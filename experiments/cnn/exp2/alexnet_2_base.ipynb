{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Segundo experimento de detecção de Alzermir com dois dataset"
      ],
      "metadata": {
        "id": "Fe_tqzoojYuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import io\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torchvision import transforms, models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import csv  # Adicionando import faltante\n",
        "import re\n",
        "\n",
        "# Mapeia os rótulos numéricos para nomes das classes (apenas para referência posterior)\n",
        "label_map = {\n",
        "    0: 'Mild_Dementia',\n",
        "    1: 'Moderate_Dementia',\n",
        "    2: 'Non_Demented',\n",
        "    3: 'Very_mild_Dementia'\n",
        "}\n",
        "\n",
        "# Faz o download da versão mais recente do dataset via kagglehub\n",
        "path = kagglehub.dataset_download(\"borhanitrash/alzheimer-mri-disease-classification-dataset\")\n",
        "\n",
        "# Mapeia os nomes das pastas para rótulos numéricos\n",
        "class_map = {\n",
        "    'Mild Dementia': 0,\n",
        "    'Moderate Dementia': 1,\n",
        "    'Non Demented': 2,\n",
        "    'Very mild Dementia': 3\n",
        "}\n",
        "\n",
        "# Cria um dataframe a partir das imagens contidas nas pastas\n",
        "def criar_dataframe_de_imagens(base_path, class_map, source_name):\n",
        "    dados = []\n",
        "    for label_nome, label_id in class_map.items():\n",
        "        pasta = os.path.join(base_path, label_nome)\n",
        "        if not os.path.exists(pasta):\n",
        "            continue\n",
        "        for nome_arquivo in os.listdir(pasta):\n",
        "            caminho_completo = os.path.join(pasta, nome_arquivo)\n",
        "            dados.append({\n",
        "                'image_path': caminho_completo,\n",
        "                'label': label_id,\n",
        "                'source': source_name\n",
        "            })\n",
        "    return pd.DataFrame(dados)\n",
        "\n",
        "# Carrega os dados de imagem das pastas (OASIS)\n",
        "df_train_oasis = criar_dataframe_de_imagens(\n",
        "    '/kaggle/input/oasis-alzheimer-dataset/train', class_map, 'oasis_train')\n",
        "df_test_oasis = criar_dataframe_de_imagens(\n",
        "    '/kaggle/input/oasis-alzheimer-dataset/test', class_map, 'oasis_test')\n",
        "\n",
        "# Carrega os dados do arquivo Parquet\n",
        "df_parquet = pd.read_parquet('/kaggle/input/alzheimer-mri-disease-classification-dataset/Alzheimer MRI Disease Classification Dataset/Data/train-00000-of-00001-c08a401c53fe5312.parquet')\n",
        "\n",
        "# Converte os bytes da imagem em objetos BytesIO (necessário para abrir com PIL)\n",
        "df_parquet['image_path'] = df_parquet['image'].apply(lambda x: BytesIO(x['bytes']))\n",
        "\n",
        "# Seleciona apenas as colunas necessárias e adiciona a coluna 'source'\n",
        "df_parquet_final = df_parquet[['image_path', 'label']].copy()\n",
        "df_parquet_final['source'] = 'parquet'\n",
        "\n",
        "# Junta os três dataframes em um único dataframe geral\n",
        "df_geral = pd.concat([df_parquet_final, df_train_oasis, df_test_oasis], ignore_index=True)\n",
        "\n",
        "# Mostra a distribuição de amostras por fonte\n",
        "print(\"\\nDistribuição por fonte no df_geral:\\n\", df_geral['source'].value_counts())\n",
        "\n",
        "# Cirar dataset customizado\n",
        "class AlzheimerUnifiedDataset(Dataset):\n",
        "  def __init__(self, dataframe, transform=None):\n",
        "    self.df = dataframe\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    entrada = self.df.iloc[idx]['image_path']\n",
        "    label = self.df.iloc[idx]['label']\n",
        "\n",
        "    # Se for caminho (string), abra a imagem\n",
        "    if isinstance(entrada, str):\n",
        "      image = Image.open(entrada).convert('RGB')\n",
        "    else:\n",
        "      image = Image.open(entrada).convert('RGB')\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "# Split em treino, validação e teste\n",
        "df_trainval, df_test = train_test_split(df_geral, test_size=0.15, stratify=df_geral['label'], random_state=42)\n",
        "df_train, df_val = train_test_split(df_trainval, test_size=0.15, stratify=df_trainval['label'], random_state=42)\n",
        "\n",
        "# Transforms padrão\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],    # padrão ImageNet\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Carregar os loaders\n",
        "train_dataset = AlzheimerUnifiedDataset(df_train, transform=transform)\n",
        "val_dataset = AlzheimerUnifiedDataset(df_val, transform=transform)\n",
        "test_dataset = AlzheimerUnifiedDataset(df_test, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Modelo com AlexNet pré-treinado\n",
        "model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\n",
        "\n",
        "# Substitui a camada final para 4 classes\n",
        "# A AlexNet tem um classifier com vários Layers\n",
        "# Vamos modificar a útltima camada linear (classifier[6])\n",
        "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 4)\n",
        "\n",
        "# Colocar o modelo na GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# Função de perda com pesos\n",
        "weights = torch.tensor([1.0, 3.5, 1.0, 1.0]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Treinamento\n",
        "num_epochs = 10\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "start_time = time.time()  # CORREÇÃO: star_time -> start_time\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    y_train_true, y_train_pred = [], []\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=f'Época {epoch + 1}/{num_epochs} - Treino'):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # Métricas treino\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "      y_train_true.extend(labels.cpu().numpy())\n",
        "      y_train_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train_true, y_train_pred)\n",
        "    train_precision = precision_score(y_train_true, y_train_pred, average='weighted', zero_division=0)\n",
        "    train_recall = recall_score(y_train_true, y_train_pred, average='weighted', zero_division=0)\n",
        "    train_f1 = f1_score(y_train_true, y_train_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    # Validação\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    y_val_true, y_val_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for images, labels in tqdm(val_loader, desc=f'Época {epoch+1}/{num_epochs} - Validação'):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        # Métricas validação\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        y_val_true.extend(labels.cpu().numpy())\n",
        "        y_val_pred.extend(preds.cpu().numpy())\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracy = accuracy_score(y_val_true, y_val_pred)\n",
        "    val_precision = precision_score(y_val_true, y_val_pred, average='weighted', zero_division=0)\n",
        "    val_recall = recall_score(y_val_true, y_val_pred, average='weighted', zero_division=0)\n",
        "    val_f1 = f1_score(y_val_true, y_val_pred, average='weighted', zero_division=0)\n",
        "    print(f\"\"\"\n",
        "    Época {epoch+1}/{num_epochs}\n",
        "    ----------------------------\n",
        "    Treino:\n",
        "    Loss: {avg_train_loss:.4f}\n",
        "    Acurácia: {train_accuracy:.4f}\n",
        "    Precisão: {train_precision:.4f}\n",
        "    Recall: {train_recall:.4f}\n",
        "    F1 Score: {train_f1:.4f}\n",
        "    Validação:\n",
        "    Loss: {avg_val_loss:.4f}\n",
        "    Acurácia: {val_accuracy:.4f}\n",
        "    Precisão: {val_precision:.4f}\n",
        "    Recall: {val_recall:.4f}\n",
        "    F1 Score: {val_f1:.4f}\n",
        "    \"\"\")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Tempo de execução: {execution_time} segundos\")\n",
        "\n",
        "resultados = {\n",
        "    \"modelo\": \"AlexNet\",\n",
        "    \"epocas\": num_epochs,\n",
        "    \"tempo_total\": round(execution_time, 2), # em segundos\n",
        "    \"acuracia\": round(train_accuracy, 4),\n",
        "    \"precisao\": round(train_precision, 4),\n",
        "    \"revocacao\": round(train_recall, 4),\n",
        "    \"f1_macro\": round(train_f1, 4)\n",
        "}\n",
        "\n",
        "def limpar_nome_arquivo(nome):\n",
        "    return re.sub(r'[\\/:\"*?<>|]+', \"_\", nome)\n",
        "\n",
        "nome_seguro = limpar_nome_arquivo(\"AlexNet\")  # CORREÇÃO: usando string em vez de variável não definida\n",
        "\n",
        "file_path = f\"resultados_do_modelo_{nome_seguro}.csv\"\n",
        "campos = list(resultados.keys())\n",
        "\n",
        "# Cria o arquivo se não existir\n",
        "if not os.path.exists(file_path):\n",
        "    with open(file_path, 'w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=campos)\n",
        "        writer.writeheader()\n",
        "        writer.writerow(resultados)\n",
        "else:\n",
        "    # Apenas adiciona nova linha\n",
        "    with open(file_path, 'a', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=campos)\n",
        "        writer.writerow(resultados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woq_0qWdJz2H",
        "outputId": "f6b9992c-3bd3-472a-fc7c-d0f43d057d22"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Distribuição por fonte no df_geral:\n",
            " source\n",
            "parquet    5120\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 1/10 - Treino: 100%|██████████| 232/232 [00:15<00:00, 15.38it/s]\n",
            "Época 1/10 - Validação: 100%|██████████| 41/41 [00:01<00:00, 28.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Época 1/10\n",
            "    ----------------------------\n",
            "    Treino:\n",
            "    Loss: 1.0122\n",
            "    Acurácia: 0.5588\n",
            "    Precisão: 0.5274\n",
            "    Recall: 0.5588\n",
            "    F1 Score: 0.5256\n",
            "    Validação:\n",
            "    Loss: 0.9425\n",
            "    Acurácia: 0.5299\n",
            "    Precisão: 0.6803\n",
            "    Recall: 0.5299\n",
            "    F1 Score: 0.4209\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 2/10 - Treino: 100%|██████████| 232/232 [00:16<00:00, 14.45it/s]\n",
            "Época 2/10 - Validação: 100%|██████████| 41/41 [00:01<00:00, 22.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Época 2/10\n",
            "    ----------------------------\n",
            "    Treino:\n",
            "    Loss: 0.8416\n",
            "    Acurácia: 0.6310\n",
            "    Precisão: 0.6272\n",
            "    Recall: 0.6310\n",
            "    F1 Score: 0.6228\n",
            "    Validação:\n",
            "    Loss: 0.8597\n",
            "    Acurácia: 0.6187\n",
            "    Precisão: 0.6516\n",
            "    Recall: 0.6187\n",
            "    F1 Score: 0.5907\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 3/10 - Treino: 100%|██████████| 232/232 [00:15<00:00, 15.44it/s]\n",
            "Época 3/10 - Validação: 100%|██████████| 41/41 [00:01<00:00, 28.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Época 3/10\n",
            "    ----------------------------\n",
            "    Treino:\n",
            "    Loss: 0.6873\n",
            "    Acurácia: 0.7018\n",
            "    Precisão: 0.7047\n",
            "    Recall: 0.7018\n",
            "    F1 Score: 0.7025\n",
            "    Validação:\n",
            "    Loss: 0.6574\n",
            "    Acurácia: 0.6983\n",
            "    Precisão: 0.7363\n",
            "    Recall: 0.6983\n",
            "    F1 Score: 0.6892\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 4/10 - Treino: 100%|██████████| 232/232 [00:15<00:00, 15.24it/s]\n",
            "Época 4/10 - Validação: 100%|██████████| 41/41 [00:01<00:00, 27.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Época 4/10\n",
            "    ----------------------------\n",
            "    Treino:\n",
            "    Loss: 0.4529\n",
            "    Acurácia: 0.8148\n",
            "    Precisão: 0.8156\n",
            "    Recall: 0.8148\n",
            "    F1 Score: 0.8148\n",
            "    Validação:\n",
            "    Loss: 0.5625\n",
            "    Acurácia: 0.7626\n",
            "    Precisão: 0.8008\n",
            "    Recall: 0.7626\n",
            "    F1 Score: 0.7683\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 5/10 - Treino: 100%|██████████| 232/232 [00:15<00:00, 15.36it/s]\n",
            "Época 5/10 - Validação: 100%|██████████| 41/41 [00:01<00:00, 21.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Época 5/10\n",
            "    ----------------------------\n",
            "    Treino:\n",
            "    Loss: 0.3155\n",
            "    Acurácia: 0.8794\n",
            "    Precisão: 0.8801\n",
            "    Recall: 0.8794\n",
            "    F1 Score: 0.8796\n",
            "    Validação:\n",
            "    Loss: 0.4495\n",
            "    Acurácia: 0.8239\n",
            "    Precisão: 0.8452\n",
            "    Recall: 0.8239\n",
            "    F1 Score: 0.8285\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 6/10 - Treino: 100%|██████████| 232/232 [00:15<00:00, 15.01it/s]\n",
            "Época 6/10 - Validação: 100%|██████████| 41/41 [00:01<00:00, 28.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Época 6/10\n",
            "    ----------------------------\n",
            "    Treino:\n",
            "    Loss: 0.2298\n",
            "    Acurácia: 0.9097\n",
            "    Precisão: 0.9099\n",
            "    Recall: 0.9097\n",
            "    F1 Score: 0.9097\n",
            "    Validação:\n",
            "    Loss: 0.4958\n",
            "    Acurácia: 0.8178\n",
            "    Precisão: 0.8364\n",
            "    Recall: 0.8178\n",
            "    F1 Score: 0.8092\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 7/10 - Treino: 100%|██████████| 232/232 [00:15<00:00, 15.39it/s]\n",
            "Época 7/10 - Validação: 100%|██████████| 41/41 [00:01<00:00, 28.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Época 7/10\n",
            "    ----------------------------\n",
            "    Treino:\n",
            "    Loss: 0.1409\n",
            "    Acurácia: 0.9492\n",
            "    Precisão: 0.9492\n",
            "    Recall: 0.9492\n",
            "    F1 Score: 0.9492\n",
            "    Validação:\n",
            "    Loss: 0.4137\n",
            "    Acurácia: 0.8760\n",
            "    Precisão: 0.8837\n",
            "    Recall: 0.8760\n",
            "    F1 Score: 0.8739\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 8/10 - Treino: 100%|██████████| 232/232 [00:15<00:00, 15.40it/s]\n",
            "Época 8/10 - Validação: 100%|██████████| 41/41 [00:01<00:00, 28.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Época 8/10\n",
            "    ----------------------------\n",
            "    Treino:\n",
            "    Loss: 0.1433\n",
            "    Acurácia: 0.9448\n",
            "    Precisão: 0.9450\n",
            "    Recall: 0.9448\n",
            "    F1 Score: 0.9449\n",
            "    Validação:\n",
            "    Loss: 0.2653\n",
            "    Acurácia: 0.9020\n",
            "    Precisão: 0.9043\n",
            "    Recall: 0.9020\n",
            "    F1 Score: 0.9003\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 9/10 - Treino: 100%|██████████| 232/232 [00:15<00:00, 15.29it/s]\n",
            "Época 9/10 - Validação: 100%|██████████| 41/41 [00:02<00:00, 20.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Época 9/10\n",
            "    ----------------------------\n",
            "    Treino:\n",
            "    Loss: 0.1258\n",
            "    Acurácia: 0.9535\n",
            "    Precisão: 0.9537\n",
            "    Recall: 0.9535\n",
            "    F1 Score: 0.9535\n",
            "    Validação:\n",
            "    Loss: 0.2298\n",
            "    Acurácia: 0.9234\n",
            "    Precisão: 0.9261\n",
            "    Recall: 0.9234\n",
            "    F1 Score: 0.9239\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 10/10 - Treino: 100%|██████████| 232/232 [00:15<00:00, 15.38it/s]\n",
            "Época 10/10 - Validação: 100%|██████████| 41/41 [00:01<00:00, 28.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Época 10/10\n",
            "    ----------------------------\n",
            "    Treino:\n",
            "    Loss: 0.0414\n",
            "    Acurácia: 0.9851\n",
            "    Precisão: 0.9852\n",
            "    Recall: 0.9851\n",
            "    F1 Score: 0.9851\n",
            "    Validação:\n",
            "    Loss: 0.3976\n",
            "    Acurácia: 0.9142\n",
            "    Precisão: 0.9178\n",
            "    Recall: 0.9142\n",
            "    F1 Score: 0.9131\n",
            "    \n",
            "Tempo de execução: 168.57792115211487 segundos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}